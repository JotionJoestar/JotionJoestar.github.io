[
        {
                "name": "QQP",
                "description": "The Quora Question Pairs2 dataset is a collection of question pairs from the community question-answering website Quora. The task is to determine whether a pair of questions are semantically equivalent.",
                "available_transformation_type": [
                        "domain",
                        "ut",
                        "domain_domain",
                        "domain_ut",
                        "ut_ut"
                ],
                "dataset_size": 40430,
                "models": [
                        {
                                "model_name": "bert-base-uncased",
                                "paper_link": "https://arxiv.org/abs/1810.04805",
                                "github_link": "https://github.com/google-research/bert",
                                "paper_name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                                "metric": {
                                        "Accuracy": 90.91
                                }
                        },
                        {
                                "model_name": "bert-large-uncased",
                                "paper_link": "https://arxiv.org/abs/1810.04805",
                                "github_link": "https://github.com/google-research/bert",
                                "paper_name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                                "metric": {
                                        "Accuracy": 90.98
                                }
                        },
                        {
                                "model_name": "xlnet-base-cased",
                                "paper_link": "https://arxiv.org/abs/1906.08237",
                                "github_link": "https://github.com/zihangdai/xlnet",
                                "paper_name": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                                "metric": {
                                        "Accuracy": 90.66
                                }
                        },
                        {
                                "model_name": "xlnet-large-cased",
                                "paper_link": "https://arxiv.org/abs/1906.08237",
                                "github_link": "https://github.com/zihangdai/xlnet",
                                "paper_name": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                                "metric": {
                                        "Accuracy": 90.79
                                }
                        },
                        {
                                "model_name": "roberta-base",
                                "paper_link": "https://arxiv.org/abs/1907.11692",
                                "github_link": "https://github.com/pytorch/fairseq/tree/master/examples/roberta",
                                "paper_name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                                "metric": {
                                        "Accuracy": 91.41
                                }
                        },
                        {
                                "model_name": "roberta-large",
                                "paper_link": "https://arxiv.org/abs/1907.11692",
                                "github_link": "https://github.com/pytorch/fairseq/tree/master/examples/roberta",
                                "paper_name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                                "metric": {
                                        "Accuracy": 92.03
                                }
                        },
                        {
                                "model_name": "albert-base-v2",
                                "paper_link": "https://arxiv.org/abs/1909.11942",
                                "github_link": "https://github.com/google-research/albert",
                                "paper_name": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
                                "metric": {
                                        "Accuracy": 90.73
                                }
                        },
                        {
                                "model_name": "albert-large-v2",
                                "paper_link": "https://arxiv.org/abs/1909.11942",
                                "github_link": "https://github.com/google-research/albert",
                                "paper_name": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
                                "metric": {
                                        "Accuracy": 90.91
                                }
                        },
                        {
                                "model_name": "albert-xxlarge-v2",
                                "paper_link": "https://arxiv.org/abs/1909.11942",
                                "github_link": "https://github.com/google-research/albert",
                                "paper_name": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
                                "metric": {
                                        "Accuracy": 92.28
                                }
                        },
                        {
                                "model_name": "distilbert-base-cased",
                                "paper_link": "https://arxiv.org/abs/1910.01108",
                                "github_link": "https://github.com/huggingface/transformers/tree/master/examples/distillation",
                                "paper_name": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                                "metric": {
                                        "Accuracy": 89.73
                                }
                        }
                ]
        },
        {
                "name": "MRPC",
                "description": " The Microsoft Research Paraphrase Corpus (Dolan & Brockett, 2005) is a corpus of sentence pairs automatically extracted from online news sources, with human annotations for whether the sentences in the pair are semantically equivalent.",
                "available_transformation_type": [
                        "domain",
                        "ut",
                        "domain_domain",
                        "domain_ut",
                        "ut_ut"
                ],
                "dataset_size": 1725,
                "models": [
                        {
                                "model_name": "bert-base-uncased",
                                "paper_link": "https://arxiv.org/abs/1810.04805",
                                "github_link": "https://github.com/google-research/bert",
                                "paper_name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                                "metric": {
                                        "Accuracy": 85.78
                                }
                        },
                        {
                                "model_name": "bert-large-uncased",
                                "paper_link": "https://arxiv.org/abs/1810.04805",
                                "github_link": "https://github.com/google-research/bert",
                                "paper_name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                                "metric": {
                                        "Accuracy": 87.75
                                }
                        },
                        {
                                "model_name": "xlnet-base-cased",
                                "paper_link": "https://arxiv.org/abs/1906.08237",
                                "github_link": "https://github.com/zihangdai/xlnet",
                                "paper_name": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                                "metric": {
                                        "Accuracy": 88.73
                                }
                        },
                        {
                                "model_name": "xlnet-large-cased",
                                "paper_link": "https://arxiv.org/abs/1906.08237",
                                "github_link": "https://github.com/zihangdai/xlnet",
                                "paper_name": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                                "metric": {
                                        "Accuracy": 88.73
                                }
                        },
                        {
                                "model_name": "roberta-base",
                                "paper_link": "https://arxiv.org/abs/1907.11692",
                                "github_link": "https://github.com/pytorch/fairseq/tree/master/examples/roberta",
                                "paper_name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                                "metric": {
                                        "Accuracy": 89.70
                                }
                        },
                        {
                                "model_name": "roberta-large",
                                "paper_link": "https://arxiv.org/abs/1907.11692",
                                "github_link": "https://github.com/pytorch/fairseq/tree/master/examples/roberta",
                                "paper_name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                                "metric": {
                                        "Accuracy": 90.90
                                }
                        },
                        {
                                "model_name": "albert-base-v2",
                                "paper_link": "https://arxiv.org/abs/1909.11942",
                                "github_link": "https://github.com/google-research/albert",
                                "paper_name": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
                                "metric": {
                                        "Accuracy": 88.97
                                }
                        },
                        {
                                "model_name": "albert-large-v2",
                                "paper_link": "https://arxiv.org/abs/1909.11942",
                                "github_link": "https://github.com/google-research/albert",
                                "paper_name": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
                                "metric": {
                                        "Accuracy": 90.44
                                }
                        },
                        {
                                "model_name": "albert-xxlarge-v2",
                                "paper_link": "https://arxiv.org/abs/1909.11942",
                                "github_link": "https://github.com/google-research/albert",
                                "paper_name": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
                                "metric": {
                                        "Accuracy": 90.68
                                }
                        },
                        {
                                "model_name": "distilbert-base-cased",
                                "paper_link": "https://arxiv.org/abs/1910.01108",
                                "github_link": "https://github.com/huggingface/transformers/tree/master/examples/distillation",
                                "paper_name": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter",
                                "metric": {
                                        "Accuracy": 84.56
                                }
                        }
                ]
        }
        
]