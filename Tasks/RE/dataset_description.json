[
  {
      "name": "Tacred",
      "description":  "Tacred is a relation extraction dataset with around 15,000 sentences",
      "available_transformation_type":  ["doman", "ut", "domain_domain", "domain_ut", "ut_ut", "subpopulation"],
      "models": [{
      "model_name": "LSTM-ATT",
      "paper_link": "https://www.aclweb.org/anthology/D17-1004.pdf",
      "github_link":"https://github.com/yuhaozhang/tacred-relation",
      "paper_name":"Position-aware Attention and Supervised Data Improve Slot Filling",
      "metric":{
          "F1":65.4
          }
        },
        {"model_name": "GCN",
        "paper_link":"https://www.aclweb.org/anthology/D18-1244.pdf",
        "github_link":"https://github.com/qipeng/gcn-over-pruned-trees",
        "paper_name": "Graph Convolution over Pruned Dependency Trees Improves Relation Extraction",
        "metric":{
        "F1": 62.03
        }
        },
          {"model_name": "AGGCN",
          "paper_link": "https://www.aclweb.org/anthology/P19-1024.pdf",
          "github_link":"https://github.com/Cartus/AGGCN",
          "paper_name":"Attention Guided Graph Convolutional Networks for Relation Extraction",
          "metric":{
          "F1": 67.41
          }
        },
          {"model_name":"BERT-base-uncased",
          "paper_link":"https://arxiv.org/abs/1810.04805",
          "github_link":"https://github.com/huggingface/transformers",
          "paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "metric":{
              "F1": "68.01"
          }
        },
          {"model_name":"BERT-large-uncased",
          "paper_link":"https://arxiv.org/abs/1810.04805",
          "github_link":"https://github.com/huggingface/transformers",
          "paper_name":"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
          "metric":{
              "F1": "67.73"
          }
        }
        ]},
        {
          "name": "NYT",
          "description": "NYT is a distantly supervised relation extraction dataset with around 172,000 sentences",
          "available_transformation_type": [
            "domain",
            "ut",
            "domain_domain",
            "domain_ut",
            "ut_ut",
            "subpopulation"
          ],
          "models": [
            {
              "model_name": "PCNN-ATT",
              "paper_link": "https://www.aclweb.org/anthology/P16-1200.pdf",
              "github_link": "https://github.com/thunlp/OpenNRE",
              "paper_name": "Neural Relation Extraction with Selective Attention over Instances",
              "metric": {
                "AUC": 0.3366,
                "F1": 0.412
              }
            },
            {
              "model_name": "PCNN+ATT RA+BAG ATT",
              "paper_link": "https://arxiv.org/abs/1904.00143",
              "github_link": "https://github.com/ZhixiuYe/Intra-Bag-and-Inter-Bag-Attentions",
              "paper_name": "Distant Supervision Relation Extraction with Intra-Bag and Inter-Bag Attentions",
              "metric": {
                "AUC": 0.4212,
                "F1": 0.4555
              }
            },
            {
              "model_name": "PCNN",
              "paper_link": "https://www.aclweb.org/anthology/D15-1203.pdf",
              "github_link": "https://github.com/thunlp/OpenNRE",
              "paper_name": "Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks",
              "metric": {
                "AUC": 0.3463,
                "F1": 0.4087
              }
            },
            {
              "model_name": "SeG",
              "paper_link": "https://arxiv.org/pdf/1911.11899.pdf",
              "github_link": "https://github.com/tmliang/SeG",
              "paper_name": "Self-Attention Enhanced Selective Gate with Entity-Aware Embedding for Distantly Supervised Relation Extraction",
              "metric": {
                "AUC": 0.4575,
                "F1": 0.4927
              }
            },
            {
              "model_name": "DISTRE",
              "paper_link": "https://www.aclweb.org/anthology/P19-1134",
              "github_link": "https://github.com/DFKI-NLP/DISTRE",
              "paper_name": "Fine-tuning Pre-Trained Transformer Language Models to Distantly Supervised Relation Extraction",
              "metric": {
                "AUC": 0.4225,
                "F1": 0.4864
              }
            }
          ]
        }
]











