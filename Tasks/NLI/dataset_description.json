[
        {
                "name": "SNLI",
                "description": "The SNLI corpus (version 1.0) is a collection of 570k human-written English sentence pairs manually labeled for balanced classification with the labels entailment, contradiction, and neutral, supporting the task of natural language inference (NLI), also known as recognizing textual entailment (RTE). We aim for it to serve both as a benchmark for evaluating representational systems for text, especially including those induced by representation learning methods, as well as a resource for developing NLP models of any kind.",
                "available_transformation_type": [
                        "domain",
                        "ut",
                        "domain_domain",
                        "domain_ut",
                        "ut_ut"
                ],
                "dataset_size": 10000,
                "models": [
                        {
                                "model_name": "bert-base-uncased",
                                "paper_link": "https://arxiv.org/abs/1810.04805",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                                "metric": {
                                        "Accuracy": 88.99
                                }
                        },
                        {
                                "model_name": "bert-large-uncased",
                                "paper_link": "https://arxiv.org/abs/1810.04805",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                                "metric": {
                                        "Accuracy": 89.37
                                }
                        },
                        {
                                "model_name": "xlnet-base-cased",
                                "paper_link": "https://arxiv.org/abs/1906.08237",
                                "github_link": "https://github.com/zihangdai/xlnet",
                                "paper_name": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                                "metric": {
                                        "Accuracy": 89.45
                                }
                        },
                        {
                                "model_name": "xlnet-large-cased",
                                "paper_link": "https://arxiv.org/abs/1906.08237",
                                "github_link": "https://github.com/zihangdai/xlnet",
                                "paper_name": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                                "metric": {
                                        "Accuracy": 90.6
                                }
                        },
                        {
                                "model_name": "roberta-base",
                                "paper_link": "https://arxiv.org/abs/1907.11692",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                                "metric": {
                                        "Accuracy": 89.97
                                }
                        },
                        {
                                "model_name": "roberta-large",
                                "paper_link": "https://arxiv.org/abs/1907.11692",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                                "metric": {
                                        "Accuracy": 90.56
                                }
                        },
                        {
                                "model_name": "albert-base-v2",
                                "paper_link": "https://arxiv.org/abs/1909.11942",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
                                "metric": {
                                        "Accuracy": 88.3
                                }
                        },
                        {
                                "model_name": "albert-xxlarge-v2",
                                "paper_link": "https://arxiv.org/abs/1909.11942",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
                                "metric": {
                                        "Accuracy": 90.19
                                }
                        }
                ]
        },
        {
                "name": "MNLI-m",
                "description": "The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. The corpus served as the basis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.",
                "available_transformation_type": [
                        "domain",
                        "ut",
                        "domain_domain",
                        "domain_ut",
                        "ut_ut"
                ],
                "dataset_size": 9815,
                "models": [
                        {
                                "model_name": "bert-base-uncased",
                                "paper_link": "https://arxiv.org/abs/1810.04805",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                                "metric": {
                                        "Accuracy": 84.4
                                }
                        },
                        {
                                "model_name": "bert-large-uncased",
                                "paper_link": "https://arxiv.org/abs/1810.04805",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                                "metric": {
                                        "Accuracy": 86.18
                                }
                        },
                        {
                                "model_name": "xlnet-base-cased",
                                "paper_link": "https://arxiv.org/abs/1906.08237",
                                "github_link": "https://github.com/zihangdai/xlnet",
                                "paper_name": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                                "metric": {
                                        "Accuracy": 86.66
                                }
                        },
                        {
                                "model_name": "xlnet-large-cased",
                                "paper_link": "https://arxiv.org/abs/1906.08237",
                                "github_link": "https://github.com/zihangdai/xlnet",
                                "paper_name": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                                "metric": {
                                        "Accuracy": 89.03
                                }
                        },
                        {
                                "model_name": "roberta-base",
                                "paper_link": "https://arxiv.org/abs/1907.11692",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                                "metric": {
                                        "Accuracy": 87.54
                                }
                        },
                        {
                                "model_name": "roberta-large",
                                "paper_link": "https://arxiv.org/abs/1907.11692",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                                "metric": {
                                        "Accuracy": 90.6
                                }
                        },
                        {
                                "model_name": "albert-base-v2",
                                "paper_link": "https://arxiv.org/abs/1909.11942",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
                                "metric": {
                                        "Accuracy": 84.15
                                }
                        },
                        {
                                "model_name": "albert-xxlarge-v2",
                                "paper_link": "https://arxiv.org/abs/1909.11942",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
                                "metric": {
                                        "Accuracy": 89.45
                                }
                        }
                ]
        },
        {
                "name": "MNLI-mm",
                "description": "The Multi-Genre Natural Language Inference (MultiNLI) corpus is a crowd-sourced collection of 433k sentence pairs annotated with textual entailment information. The corpus is modeled on the SNLI corpus, but differs in that covers a range of genres of spoken and written text, and supports a distinctive cross-genre generalization evaluation. The corpus served as the basis for the shared task of the RepEval 2017 Workshop at EMNLP in Copenhagen.",
                "available_transformation_type": [
                        "domain",
                        "ut",
                        "domain_domain",
                        "domain_ut",
                        "ut_ut"
                ],
                "dataset_size": 9832,
                "models": [
                        {
                                "model_name": "bert-base-uncased",
                                "paper_link": "https://arxiv.org/abs/1810.04805",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                                "metric": {
                                        "Accuracy": 84.42
                                }
                        },
                        {
                                "model_name": "bert-large-uncased",
                                "paper_link": "https://arxiv.org/abs/1810.04805",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",
                                "metric": {
                                        "Accuracy": 86.36
                                }
                        },
                        {
                                "model_name": "xlnet-base-cased",
                                "paper_link": "https://arxiv.org/abs/1906.08237",
                                "github_link": "https://github.com/zihangdai/xlnet",
                                "paper_name": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                                "metric": {
                                        "Accuracy": 86.33
                                }
                        },
                        {
                                "model_name": "xlnet-large-cased",
                                "paper_link": "https://arxiv.org/abs/1906.08237",
                                "github_link": "https://github.com/zihangdai/xlnet",
                                "paper_name": "XLNet: Generalized Autoregressive Pretraining for Language Understanding",
                                "metric": {
                                        "Accuracy": 88.62
                                }
                        },
                        {
                                "model_name": "roberta-base",
                                "paper_link": "https://arxiv.org/abs/1907.11692",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                                "metric": {
                                        "Accuracy": 87.13
                                }
                        },
                        {
                                "model_name": "roberta-large",
                                "paper_link": "https://arxiv.org/abs/1907.11692",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "RoBERTa: A Robustly Optimized BERT Pretraining Approach",
                                "metric": {
                                        "Accuracy": 90.12
                                }
                        },
                        {
                                "model_name": "albert-base-v2",
                                "paper_link": "https://arxiv.org/abs/1909.11942",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
                                "metric": {
                                        "Accuracy": 84.09
                                }
                        },
                        {
                                "model_name": "albert-xxlarge-v2",
                                "paper_link": "https://arxiv.org/abs/1909.11942",
                                "github_link": "https://github.com/huggingface/transformers",
                                "paper_name": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations",
                                "metric": {
                                        "Accuracy": 89.89
                                }
                        }
                ]
        }
]